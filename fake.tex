%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Programming/Coding Assignment
% LaTeX Template
%
% This template has been downloaded from:
% http://www.latextemplates.com
%
% Original author:
% Ted Pavlic (http://www.tedpavlic.com)
%
% Note:
% The \lipsum[#] commands throughout this template generate dummy text
% to fill the template out. These commands should all be removed when 
% writing assignment content.
%
% This template uses a Perl script as an example snippet of code, most other
% languages are also usable. Configure them in the "CODE INCLUSION 
% CONFIGURATION" section.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass{article}

\usepackage{fancyhdr} % Required for custom headers
\usepackage{lastpage} % Required to determine the last page for the footer
\usepackage{extramarks} % Required for headers and footers
\usepackage[usenames,dvipsnames]{color} % Required for custom colors
\usepackage{graphicx} % Required to insert images
\usepackage{subcaption}
\usepackage{listings} % Required for insertion of code
\usepackage{courier} % Required for the courier font
\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template

% Margins
\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1} % Line spacing

% Set up the header and footer
\pagestyle{fancy}
\lhead{\hmwkAuthorName} % Top left header
\chead{\hmwkClass\ (\hmwkClassTime): \hmwkTitle} % Top center head
%\rhead{\firstxmark} % Top right header
\lfoot{\lastxmark} % Bottom left footer
\cfoot{} % Bottom center footer
\rfoot{Page\ \thepage\ of\ \protect\pageref{LastPage}} % Bottom right footer
\renewcommand\headrulewidth{0.4pt} % Size of the header rule
\renewcommand\footrulewidth{0.4pt} % Size of the footer rule

\setlength\parindent{0pt} % Removes all indentation from paragraphs

%----------------------------------------------------------------------------------------
%	CODE INCLUSION CONFIGURATION
%----------------------------------------------------------------------------------------

\definecolor{MyDarkGreen}{rgb}{0.0,0.4,0.0} % This is the color used for comments
\lstloadlanguages{Perl} % Load Perl syntax for listings, for a list of other languages supported see: ftp://ftp.tex.ac.uk/tex-archive/macros/latex/contrib/listings/listings.pdf
\lstset{language=Perl, % Use Perl in this example
	frame=single, % Single frame around code
	basicstyle=\small\ttfamily, % Use small true type font
	keywordstyle=[1]\color{Blue}\bf, % Perl functions bold and blue
	keywordstyle=[2]\color{Purple}, % Perl function arguments purple
	keywordstyle=[3]\color{Blue}\underbar, % Custom functions underlined and blue
	identifierstyle=, % Nothing special about identifiers                                         
	commentstyle=\usefont{T1}{pcr}{m}{sl}\color{MyDarkGreen}\small, % Comments small dark green courier font
	stringstyle=\color{Purple}, % Strings are purple
	showstringspaces=false, % Don't put marks in string spaces
	tabsize=5, % 5 spaces per tab
	%
	% Put standard Perl functions not included in the default language here
	morekeywords={rand},
	%
	% Put Perl function parameters here
	morekeywords=[2]{on, off, interp},
	%
	% Put user defined functions here
	morekeywords=[3]{test},
	%
	morecomment=[l][\color{Blue}]{...}, % Line continuation (...) like blue comment
	numbers=left, % Line numbers on left
	firstnumber=1, % Line numbers start with line 1
	numberstyle=\tiny\color{Blue}, % Line numbers are blue and small
	stepnumber=5 % Line numbers go in steps of 5
}

% Creates a new command to include a perl script, the first parameter is the filename of the script (without .pl), the second parameter is the caption
\newcommand{\perlscript}[2]{
	\begin{itemize}
		\item[]\lstinputlisting[caption=#2,label=#1]{#1.pl}
	\end{itemize}
}

%----------------------------------------------------------------------------------------
%	DOCUMENT STRUCTURE COMMANDS
%	Skip this unless you know what you're doing
%----------------------------------------------------------------------------------------

% Header and footer for when a page split occurs within a problem environment
\newcommand{\enterProblemHeader}[1]{
	%\nobreak\extramarks{#1}{#1 continued on next page\ldots}\nobreak
	%\nobreak\extramarks{#1 (continued)}{#1 continued on next page\ldots}\nobreak
}

% Header and footer for when a page split occurs between problem environments
\newcommand{\exitProblemHeader}[1]{
	%\nobreak\extramarks{#1 (continued)}{#1 continued on next page\ldots}\nobreak
	%\nobreak\extramarks{#1}{}\nobreak
}

\setcounter{secnumdepth}{0} % Removes default section numbers
\newcounter{homeworkProblemCounter} % Creates a counter to keep track of the number of problems
\setcounter{homeworkProblemCounter}{-1}

\newcommand{\homeworkProblemName}{}
\newenvironment{homeworkProblem}[1][Problem \arabic{homeworkProblemCounter}]{ % Makes a new environment called homeworkProblem which takes 1 argument (custom name) but the default is "Problem #"
	\stepcounter{homeworkProblemCounter} % Increase counter for number of problems
	\renewcommand{\homeworkProblemName}{#1} % Assign \homeworkProblemName the name of the problem
	\section{\homeworkProblemName} % Make a section in the document with the custom problem count
	\enterProblemHeader{\homeworkProblemName} % Header and footer within the environment
}{
	\exitProblemHeader{\homeworkProblemName} % Header and footer after the environment
}

\newcommand{\problemAnswer}[1]{ % Defines the problem answer command with the content as the only argument
	\noindent\framebox[\columnwidth][c]{\begin{minipage}{0.98\columnwidth}#1\end{minipage}} % Makes the box around the problem answer and puts the content inside
}

\newcommand{\homeworkSectionName}{}
\newenvironment{homeworkSection}[1]{ % New environment for sections within homework problems, takes 1 argument - the name of the section
	\renewcommand{\homeworkSectionName}{#1} % Assign \homeworkSectionName to the name of the section from the environment argument
	\subsection{\homeworkSectionName} % Make a subsection with the custom name of the subsection
	\enterProblemHeader{\homeworkProblemName\ [\homeworkSectionName]} % Header and footer within the environment
}{
	\enterProblemHeader{\homeworkProblemName} % Header and footer after the environment
}

%----------------------------------------------------------------------------------------
%	NAME AND CLASS SECTION
%----------------------------------------------------------------------------------------

\newcommand{\hmwkTitle}{Project 2} % Assignment title
\newcommand{\hmwkDueDate}{Sunday,\ February\ 18,\ 2018} % Due date
\newcommand{\hmwkClass}{CSC411} % Course/class
\newcommand{\hmwkClassTime}{L0101} % Class/lecture time
\newcommand{\hmwkAuthorName}{Ying Yang} % Your name

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\title{
	\vspace{2in}
	\textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
	\normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate}\\
	\vspace{0.1in}
	\vspace{3in}
}

\author{\textbf{\hmwkAuthorName}}
%\date{} % Insert date here if you want it to appear below your name

%----------------------------------------------------------------------------------------

\begin{document}
	
	\maketitle
	\clearpage
	%----------------------------------------------------------------------------------------
	%	PROBLEM 1
	%----------------------------------------------------------------------------------------
	
	% To have just one problem per page, simply put a \clearpage after each problem
	
	\begin{homeworkProblem}[ Part 1]
		
		\noindent \textit{Dataset description}
		$\newline$
		
		Describe the datasets.
		
		The dataset consists of healines of real news and headlines of fake news, each stored in a separate file. All headlines' punctuations and apostrophes are removed so that it only contain clean words.\\
		
		We have a dataset of 3266 headlines
		
		real news headlines: 1968
		
		fake news headlines: 1298\\
		
		3 examples of words that may be useful\\
		
		We collected words that appear frequently in real and not frequently in fake news, and vice versa, since if a word appears frequently in a one and not frequent in the other, then the word is will strongly predict the headline. Among the results, we selected words that are meaningful (we didn't select words that are english grammar words, like "the", "and" , etc.)\\
		
		examples of words:
		
		word that appears frequent in real, and less frequent in fake: ['criticism', 10, 1]
		
		words that appear frequent in fake, and less frequent in real: 
		['reporter', 15, 1]  
		['liberty', 12, 1]\\
		
		Test scripts of the project:
		
		For the following problems, test scripts are run simply by uncommenting out the part \# in the $main()$ function.
		
			\begin{lstlisting}[language=Python, caption=Test scripts]		
			
			if __name__ == "__main__":
				# part1()
				# part2()
				# part3a()
				# part3b()
				# part4()
				# part5()
				# part6()
				# part7()
				# part8()
			\end{lstlisting}
		
		$\newline$

	\end{homeworkProblem}
	\clearpage
	%----------------------------------------------------------------------------------------
	%	PROBLEM 2
	%----------------------------------------------------------------------------------------
	
	\begin{homeworkProblem}[Part 2]
		
		\noindent \textit{Problem: Implement the Naive Bayes algorithm for predicting whether a headline is real or fake. }
		
		
		$\newline$
		Implementation details:
		We used naive bayes probability to predict whether a headline is real or fake.
		For each of the headline, we used naive bayes to calculate P(real $\mid$ headline) and P(fake $\mid$ headline) and compare which one has greater probability. For example, if P(real $\mid$ headline) $>$ P(fake $\mid$ headline), then we predict it to be real. We check if the prediction is correct using the corresponding label. We count the number of correct predictions for all headlines and divide it by the total number of headlines in each of training\_set, validation\_set and test\_test to get the performance.\\
		
		In particular, we used the formula:
		
		P(real $\mid$ headline)  = likelihood\_probability(headline) * prior\_probability(real)
		
		P(fake $\mid$ headline) = likelihood\_probability(headline) * prior\_probability(fake)
		
		
		
		= P(real $\mid$ headline)  = $\sum log(P(w_i | real)) $+  log(prior\_probability(real))
		
		= P(fake $\mid$ headline)  = $\sum log(P(w_i | fake)) $+  log(prior\_probability(fake)) \\
		
		\textbf{Step 1} : Calculate the likelihood of a headline.
		
		(similar way for getting likelihood for fake news headline)\\
		
		\textbf{Step 1.1} We first calculate the total number of words that are in real and fake news in the dataset. \\
		
		\textbf{Step 1.2} We then maintained a table of real and fake words and its counts. (in python dictionary) For each word in the dataset, we store the word as key and P(word $\mid$ real) and P(word $\mid$ fake) as a list of value of the word.
		
		P(word $\mid$ c) = (count(word, c) + m * p\_hat) / (count(c) + m) , c = real or fake
		
		m and p\_hat are the parameters that we will tune with validation set to get best performance.
		
		We used validation set to tune the parameters m and p
		
				Table of probability for each word  (P(w $\mid$ real), P(w $\mid$ fake))
				
	\begin{lstlisting}[language=Python, caption=Make a table of probability for each word]		
	probability_dict = {}
	for word in probability_dict:
		# get real words probability
		if word in words_occurrences_real_table:
			real_word_likelihood = float(words_occurrences_real_table[word] + m * p_hat) /
			 float(total_real_healines + m)
		else:
			real_word_likelihood = float(m * p_hat) / float(total_real_healines + m)
			probability_dict[word] = [real_word_likelihood]
		# get fake words probability
		if word in words_occurrences_fake_table:
			fake_word_likelihood = float(words_occurrences_fake_table[word] + m * p_hat) /
			 float(total_fake_healines + m)
		else:
			fake_word_likelihood = float(m * p_hat) / float(total_fake_healines + m)
			probability_dict[word].append(fake_word_likelihood)
	\end{lstlisting}
		
	
		$\newline$
		Step 1.3 Calculate likelihood of the healine. We do so by adding log(p(word $\mid$ c)) if word appears in the headline, log(1 - p(word $\mid$ c)) if words doesn't appear in the headline, for each of the words in the table. This sum is the likelihood of the headline. We will called this likelihood\_headline
		\begin{lstlisting}[language=Python, caption=Calculate likelihood for a headline]	
		likelihood_headline_real = 0
		likelihood_headline_fake = 0
		
		for word in likelihood_table:
			if word in headline:
				likelihood_headline_real += math.log(likelihood_table[word][0])
				likelihood_headline_fake += math.log(likelihood_table[word][1])
			else:
				likelihood_headline_real += math.log(1 - likelihood_table[word][0])
				likelihood_headline_fake += math.log(1 - likelihood_table[word][1])
		\end{lstlisting}
		
		\textbf{Step 2}: Calculate prior probability of real or fake. 
		
		P(real) = total number of headlines real / total headlines
		P(fake) = total number of headlines fake / total headlines\\
		
		\textbf{Step 3}: Calculate the posterior probability for a headline being real using likelihood\_headline + log(prior(real)). We do similar operation for calculating posterior probability for a headline being fake. \\
		
		\textbf{Step 4}: Count the accuracy. 
		
		We use the posterior probability for each headline being real or fake and compare which one is greater, and use the one that is greater to predict the whether a headline is real or fake. For example, if a headline's posterior probability for being real is greater than the posterior probability that of being fake, then we predict it is real. We iterate through all the headlines and check with the label of it. We increment correct guesses number if prediction is correct. \\
\begin{lstlisting}[language=Python, caption=Calculate performance for a dataset]	
	correct = 0
	for i in range(len(dataset)):
		headline = dataset[i]
		if (predict_headline(likelihood_table, headline, dataset, dataset_label) == 
		"real" and dataset_label[i] == 1) or\
		(predict_headline(likelihood_table, headline, dataset, dataset_label) ==
		 "fake" and dataset_label[i] == 0):
		correct += 1
	
	return (float(correct) / len(dataset)) * 100
\end{lstlisting}
		\textbf{Step 5}: We tune the parameters m and p\_hat using validation set. 
		
		To do so, we tried with an array of numbers 
		
		m = [1.0, 0.5, 1e1,1e2,1e3,1e4] ,	p\_hat = [0.9,3e-1,1e-1,3e-2,1e-2,3e-3]
		
		After several tests and changing the values for p\_hat and m, we found out that m=0.5 and p\_hat = 0.1 are the best parameters. \\
		
		Results:
		
		training set performance: 97.8127734033\%
		
		validation set performance: 99.387755102\%
		
		test set performance: 98.0592441267\%

		
	\end{homeworkProblem}
	\clearpage
	%----------------------------------------------------------------------------------------
	%	PROBLEM 3
	%----------------------------------------------------------------------------------------
	
	\begin{homeworkProblem}[Part 3]
		
		\begin{homeworkProblem}[Part 3a]
			
			How we obtained the lists:
			
			We have maintaine a table of probabilities for each words (P(w $\mid$ real) and P(w $\mid$ fake). For each of the word, we calculate store its posterior probability bydoing:\\
			
			Presence predict real: 
			P(real $\mid$ w) =  P(w $\mid$ real) + log(prior\_prob(real))
			
			Presence predict fake:
			P(fake $\mid$ w) =  P(w $\mid$ fake) + log(prior\_prob(fake))
			
			Absence predict real:
			P(real $\mid$ w) =  1 - P(w $\mid$ fake) + log(prior\_prob(real))
			
			Absence predict fake:
			P(fake $\mid$ w) =  1 - P(w $\mid$ fake) + log(prior\_prob(fake))\\
			
			Then we iterate through the table to find the top 10 highest values.
			
			$\newline$
			
			List the 10 words whose presence most strongly predicts that the news is real:
			('travel')
			
			('turnbull')
			
			('ban')
			
			('korea')
			
			('north')
			
			('says')
			
			('us')
			
			('trumps')
			
			('trump')
			
			('donald')
			
			$\newline$
			
			List the 10 words whose absence most strongly predicts that the news is real:	
			
			('if')
			
			('are')
			
			('you')
			
			('clinton')
			
			('and')
			
			('just')
			
			('is')
			
			('a')
			
			('hillary')
			
			('the')
			
			$\newline$
			List the 10 words whose presence most strongly predicts that the news is fake:
			
			('if')
			
			('are')
			
			('you')
			
			('clinton')
			
			('and')
			
			('just')
			
			('is')
			
			('a')
			
			('hillary')
			
			('the')
			
			$\newline$
			List the 10 words whose absence most strongly predicts that the news is fake:
			
			('travel')
			
			('turnbull')
			
			('ban')
			
			('korea')
			
			('north')
			
			('says')
			
			('us')
			
			('trumps')
			
			('trump')
			
			('donald')
			
			$\newline$
			Compare the influence of presence vs absence of words on predicting whether the headline is real or fake news.
			
			
			
		\end{homeworkProblem}
		
		\begin{homeworkProblem}[Part 3b]
			\noindent \textit{Write vectorized code that computes the gradient of the cost function with respect to the weights and biases of the network}
			$\newline$
			
			10 non-stopwords that most strongly predict that the news is real:
			('australia')
			('wall')
			('travel')
			('turnbull')
			('ban')
			('korea')
			('north')
			('says')
			('trumps')
			('donald')
			
			10 non-stopwords that most strongly predict that the news is fake:
			('black')
			('watch')
			('win')
			('obama')
			('new')
			('america')
			('just')
			('clinton')
			('hillary')
			('trump')
			
		\end{homeworkProblem}
		
		\begin{homeworkProblem}[Part 3c]
			Why might it make sense to remove stop words when interpreting the model? 
			
			Because stop words usually doesn't have any meaning. They are only words that constructs syntactical sentences, but doesn't contribute to the meaning of the headline. Since we want to know which are the words that predict strongly, we need only consider words that make sense. \\
			
			Why might it make sense to keep stop words?
			
			It makes sense when sometimes a fake news might tend to use a set of non-stop english words to make up the title. For example, in real news, one might not use "seems", whereas in fake news one might use the word "seems" a lot when making up a title "that creates false statements", without proofs, such as "Trump seems to have intention to declare war". In this case, real news might not use this word "seems", since they tend to only use solid proofs, and we rarely see "seems" in titles with solid proofs.
			
			
		\end{homeworkProblem}
		
	\end{homeworkProblem}
	\clearpage
	
	%----------------------------------------------------------------------------------------
	%	PROBLEM 4
	%----------------------------------------------------------------------------------------
	
	\begin{homeworkProblem}[Part 4]
Input of the model is a 2D matrix, where row size is the number of headlines words, and column size is total headlines size. Output of the model is a n by 1 vector.
\\
Learning rate: 0.1\\
Cost function: Cross-entropy\\
Regularization term: 3e-4,1e-3,3e-3,1e-2,3e-2,1e-1 were tested and 3e-3 was the optimal choice\\

\begin{figure*}[!ht]
\centering
\includegraphics[scale = 0.7 ]{learning_curve_part4.png}
\caption{Learning curve}
\label{fig:learning curve(part4)}
\end{figure*}

Accuracy of each set:\\
Training set accuracy 1.0\\
Validation set accuracy 0.830612244898\\
Test set accuracy 0.821246169561\\

The cost function of the model we use is:
$$-\sum_j(y_jlog(\theta^TX_j)+(1-y_j)log(1-\theta^TX_j)) + \lambda \Vert \theta^2 \Vert$$

\begin{figure*}[!ht]
\centering
\includegraphics[scale = 0.7 ]{tuned_parameter_part4.png}
\caption{Validation set accuracy with respect to various regularization parameter}
\label{fig:tuning parameter(part4)}
\end{figure*}

As is stated, 3e-3 got the best performance.

\end{homeworkProblem}

	\clearpage
	
	%----------------------------------------------------------------------------------------
	%	PROBLEM 5
	%----------------------------------------------------------------------------------------
	
	\begin{homeworkProblem}[Part 5]
		
		\noindent \textit{Explain what are the $\theta$s and I's in the formula below }\\
		
		$\theta_{0}$ + $\theta_1I_1(x)$ +$\theta_2I_2(x)
		$+ $\theta_kI_k(x)$ $>$ thr \\
		
		
		k represents the total number of words that appear in the headline (only unique words in each of the headline)\\
		
		Naive Bayes \\
		
		$I_i(x)$ = 1 if word appears in the headline, and 0 if it doesn't appear in the headline.
		
		$\theta_i$ = log $\frac{P(x = 1 \mid y = real)}{P(x = 1 \mid y = fake)}$ - log $\frac{P(x = 0 \mid y = real)}{P(x = 0 \mid y = fake)}$ \\
		
		Logistic regression \\
		
		I is a k * 1 vector of 1 or 0s. $I_i(x)$ = 1 if word appears in the headline, and 0 if it doesn't appear in the headline.
		
		
		$\theta_i$ represent how important is a feature contributing to the final prediction of a headline being real or fake. It is the weights matrix. 
		
		
		\clearpage
		   

	\end{homeworkProblem}
	\clearpage
	%----------------------------------------------------------------------------------------
	%	PROBLEM 6
	%----------------------------------------------------------------------------------------
	
	\begin{homeworkProblem}[Part 6]
		
		
	\end{homeworkProblem}
	
	\clearpage
	%----------------------------------------------------------------------------------------
	%	PROBLEM 7
	%----------------------------------------------------------------------------------------
	
	\begin{homeworkProblem}[Part 7]
	

	

	\end{homeworkProblem}

	\clearpage
	%----------------------------------------------------------------------------------------
	%	PROBLEM 8
	%----------------------------------------------------------------------------------------
	
	\begin{homeworkProblem}[Part 8]

		
	\end{homeworkProblem}
	
	\clearpage

\end{document}
